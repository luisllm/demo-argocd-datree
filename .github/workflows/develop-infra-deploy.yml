name: "develop - Deploy infra"

on:
  pull_request:
    branches:
      - develop
    types: [opened, synchronize]
    paths-ignore:
      - 'README.md'
  workflow_dispatch:
    branches:
      - develop

permissions:
  contents: read
  pull-requests: write
  #issues: write

env:
  # Possible values: https://developer.hashicorp.com/terraform/internals/debugging
  TF_LOG: INFO
  # Credentials for deployment to AWS
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  # S3 bucket for the Terraform state
  S3_BUCKET_TF_STATE: "develop-test-llm-terraform-state"
  TF_STATE_FILE: "vpc-eks.tfstate"
  AWS_REGION: "us-east-1"
  TERRAFORM_VERSION: "1.7.0"
  # https://docs.nginx.com/nginx-ingress-controller/technical-specifications/#supported-kubernetes-versions
  NGINX_INGRESS_CONTROLLER_CHART_VERSION: "1.0.2"
  # https://artifacthub.io/packages/helm/metrics-server/metrics-server
  METRICS_SERVER_CHART_VERSION: "3.12.0"
  # https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack
  KUBE_PROMETHEUS_STACK_CHART_VERSION: "56.21.2"
  ENVIRONMENT: develop
  EKS_CLUSTER_NAME: develop-eks-cluster
  TEST_NAMESPACE: "demo-test"
  # https://docs.crossplane.io/latest/software/install/
  CROSSPLANE_CHART_VERSION: "1.15.1"
  # https://marketplace.upbound.io/providers/upbound/provider-aws-s3/v1.2.1
  UPBOUND_PROVIDER_AWS_S3_VERSION: "v1.2.1"

jobs:
  # Deploy the VPC and the EKS cluster
  deploy_vpc_and_eks:
    name: "Deploy VPC and EKS cluster"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    defaults:
      run:
        working-directory: terraform-code/vpc-eks
    outputs:
      tfplanExitCode: ${{ steps.tf-plan.outputs.exitcode }}
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: $TERRAFORM_VERSION
      
      - name: Install checkov
        run: pip install checkov    

      - name: Terraform Format
        run: terraform fmt

      - name: Terraform Init
        run: |
          terraform init \
            -backend-config "bucket=$S3_BUCKET_TF_STATE" \
            -backend-config "key=$TF_STATE_FILE"

      - name: Terraform Validate
        run: terraform validate -no-color

      - name: Fetch variable file infra.tfvars from CONFIG REPO
        uses: actions/checkout@v2
        with:
          repository: "luisllm/environments"
          ref: develop # or specify the branch, tag, or commit hash where the file resides
          path: "./environments"
      
      - name: Print variable file infra.tfvars coming from CONFIG REPO
        run: cat ../../environments/tf-config/infra.tfvars
         
      # Generates an execution plan for Terraform
      # An exit code of 0 indicated no changes, 1 a terraform failure, 2 there are pending changes.
      - name: Terraform Plan
        id: tf-plan
        run: |
          export exitcode=0
          terraform plan -var-file="../../environments/tf-config/infra.tfvars" -detailed-exitcode -no-color -out tfplan || export exitcode=$?

          echo "exitcode=$exitcode" >> $GITHUB_OUTPUT
          if [ $exitcode -eq 1 ]; then
            echo Terraform Plan Failed!
            exit 1
          else 
            exit 0
          fi

      - name: Run checkov
        run: checkov -d . --quiet --soft-fail

      # Terraform Apply
      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan

      # Notify to Slack channel if it fails
      #- name: Notify slack fail
      #    if: failure()
      #    env:
      #      SLACK_BOT_TOKEN: ${{ secrets.SLACK_NOTIFICATIONS_BOT_TOKEN }}
      #    uses: voxmedia/github-action-slack-notify-build@v1
      #    with:
      #      channel: app-alerts
      #      status: FAILED
      #      color: danger




  # Deploy nginx ingress controller to be able to expose the EKS cluster via a public AWS LB, and be able to send requests
  ingress_deployment:
    name: "Deploy nginx ingress controller"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    needs: [deploy_vpc_and_eks]
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      # Get from AWS ParameterStore the SecGroup ID that should be attached to the public AWS LB
      # The SecGroup was created by Terraform
      - name: Get SecGroup from ParameterStore
        id: discover-lb-secgroup
        run: |
          secgroup_id=$(aws ssm get-parameter --name "/$ENVIRONMENT/public-lb-secgroup-id" --query 'Parameter.Value' --output text)
          echo "SECGROUP_ID=$secgroup_id" >>$GITHUB_ENV

      - name: Update kube config
        run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

      - name: Create public-ingress namespace
        run: |
          kubectl get namespace | grep -q "^public-ingress" || kubectl create namespace public-ingress  

      - name: Deploy nginx ingress controller
        run: |
          helm upgrade --install public-ingress \
            oci://ghcr.io/nginxinc/charts/nginx-ingress \
            --version $NGINX_INGRESS_CONTROLLER_CHART_VERSION \
            --namespace public-ingress \
            --set controller.service.annotations."service\.beta\.kubernetes\.io/aws-load-balancer-security-groups"="$SECGROUP_ID"

  

  # Store in AWS ParameterStore the public LB dns name created automatically when the nginx ingress controller was deployed. It will be used to send requests
  public_ingress_lb_discovery:
    name: "Store ingress AWS public LB dns name in AWS ParameterStore"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    needs: [ingress_deployment]
    outputs:
      ingress_lb_dns_name: ${{ steps.discover-lb-dns.outputs.ingress_lb_dns_name }}
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      # Wait for 1min for the LB to get created
      - name: Wait for 1 Minute
        run: sleep 60

      - name: Discover ingress public LB DNS Name and store it in AWS ParameterStore
        id: discover-lb-dns
        run: |
          # Get the list of load balancer names
          load_balancer_names=$(aws elb describe-load-balancers --query "LoadBalancerDescriptions[].LoadBalancerName" --output text)
          
          # Iterate over each load balancer name
          for lb_name in $load_balancer_names; do
              # Retrieve tags for the current load balancer
              tags=$(aws elb describe-tags --load-balancer-names "$lb_name" --query "TagDescriptions[].Tags[?Key=='kubernetes.io/cluster/$EKS_CLUSTER_NAME'].Value" --output text)
              
              # Check if the load balancer has the desired tag
              if [ ! -z "$tags" ]; then
                  # If the tag is found, print the DNS name of the load balancer
                  ingress_lb_dns_name=$(aws elb describe-load-balancers --load-balancer-names "$lb_name" --query "LoadBalancerDescriptions[].DNSName" --output text)
                  echo "$ingress_lb_dns_name"
                  aws ssm put-parameter --name "/$ENVIRONMENT/ingress-public-load-balancer-dns" --value "$ingress_lb_dns_name" --type String --overwrite
                  echo "::set-output name=ingress_lb_dns_name::$ingress_lb_dns_name"
              fi
          done


  # Deploy Argocd
  argocd_deployment:
    name: "Deploy ArgoCD"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    defaults:
      run:
        working-directory: argocd
    needs: [deploy_vpc_and_eks, ingress_deployment, public_ingress_lb_discovery]
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      - name: Update kube config
        run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

      - name: Create argocd namespace
        run: |
          kubectl get namespace | grep -q "^argocd" || kubectl create namespace argocd
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

      # Patch ArgoCD to be able to access the ArgoCD UI via the AWS public LB and the nginx ingress controller
      # https://arnavtripathy98.medium.com/solution-how-to-deploy-argo-cd-dashboard-over-nginx-ingress-controller-926d8a540844
      - name: Patch Argocd
        run: |
          kubectl -n argocd patch configmap argocd-cmd-params-cm --patch '{"data":{"server.insecure":"true"}}'
          kubectl -n argocd rollout restart deployment argocd-server

      # Get from AWS ParameterStore the public LB dns name created automatically when the nginx ingress controller was deployed. It will be used in the Ingress object as a Host
      - name: Get AWS public LB DNS Name from ParameterStore
        id: discover-lb-dns
        run: |
          ingress_lb_dns_name=$(aws ssm get-parameter --name "/$ENVIRONMENT/ingress-public-load-balancer-dns" --query 'Parameter.Value' --output text)
          echo "INGRESS_LB_DNS=$ingress_lb_dns_name" >>$GITHUB_ENV

      - name: Create Ingress to access ArgoCD UI
        run: |
          echo "Ingress LB DNS Name: $INGRESS_LB_DNS"
          sed "s|CHANGEME|$INGRESS_LB_DNS|g" argocd-ingress.yaml > argocd-ingress-replaced.yaml
          kubectl apply -f argocd-ingress-replaced.yaml

      - name: ArgoCD admin password
        run: |
          echo "Run the following command to get the ArgoCD admin password: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d; echo"



  # Deploy ArgoCD Project and Applications
  argocd_project_and_apps_deployment:
    name: "Deploy ArgoCD Project and Applications"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    defaults:
      run:
        working-directory: argocd
    needs: [argocd_deployment]
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      - name: Update kube config
        run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

      - name: Create ecommerge namespace and ArgoCD products app
        run: |
          kubectl get namespace | grep -q "^ecommerce" || kubectl create namespace ecommerce

      - name: Create ArgoCD Project and Applications
        run: |
          kubectl apply -f argocd-project.yaml
          kubectl apply -f argocd-products-app.yaml



  # Test and verify the infra was deployed correctly
  test_infra:
    name: "Test infra"
    runs-on: ubuntu-latest
    #environment: $ENVIRONMENT
    needs: [deploy_vpc_and_eks, ingress_deployment, public_ingress_lb_discovery, argocd_deployment, argocd_project_and_apps_deployment]
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: "us-east-1"

      - name: Update kube config
        run: aws eks update-kubeconfig --name $EKS_CLUSTER_NAME

      - name: Check all nginx ingress controller PODs are running
        id: check_nginx_pods
        run: |
          kubectl get pods -n $TEST_NAMESPACE -l app.kubernetes.io/name=nginx-ingress -o jsonpath='{range .items[*]}{.status.phase}{"\n"}{end}' | grep -qv "Running" && echo "One or more pods are not in running state" && exit 1 || echo "All pods are running"



